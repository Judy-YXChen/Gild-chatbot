import streamlit as st
import time
import re
import base64
from io import BytesIO
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import json
from collections import Counter
from typing import Union

placeholderstr = "Please input your command"
user_name = "è­½å¿ƒ"
user_image = "https://www.w3schools.com/howto/img_avatar.png"

def stream_data(stream_str):
    for word in stream_str.split(" "):
        yield word + " "
        time.sleep(0.15)

def main():
    st.set_page_config(
        page_title='K-Assistant - The Residemy Agent',
        layout='wide',
        initial_sidebar_state='auto',
        menu_items={
            'Get Help': 'https://streamlit.io/',
            'Report a bug': 'https://github.com',
            'About': 'About your application: **Hello world**'
        },
        page_icon="img/favicon.ico"
    )

    st.title(f"ğŸ’¬ {user_name}'s Chatbot")

    with st.sidebar:
        selected_lang = st.selectbox("Language", ["English", "ç¹é«”ä¸­æ–‡"], index=1)
        lang_setting = st.session_state.get('lang_setting', selected_lang)
        st.session_state['lang_setting'] = lang_setting

        st_c_1 = st.container(border=True)
        with st_c_1:
            st.image("https://www.w3schools.com/howto/img_avatar.png")

    st_c_chat = st.container(border=True)

    if "messages" not in st.session_state:
        st.session_state.messages = []
    else:
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st_c_chat.chat_message(msg["role"], avatar=user_image).markdown(msg["content"])
            elif msg["role"] == "assistant":
                st_c_chat.chat_message(msg["role"]).markdown(msg["content"])
            else:
                try:
                    image_tmp = msg.get("image")
                    if image_tmp:
                        st_c_chat.chat_message(msg["role"], avatar=image_tmp).markdown(msg["content"])
                except:
                    st_c_chat.chat_message(msg["role"]).markdown(msg["content"])

    # Load data
    with open("full_record.json", "r", encoding="utf-8") as f:
        full_record = json.load(f)

    # Load stopwords
    with open("stopwords.txt", "r", encoding="utf-8") as f:
        stopwords = set([line.strip() for line in f if line.strip()])
    custom_stopwords = {"(",")","Studios", "æ··éŸ³", ":", "!", "/", "...", ".", ",", "'", "Studio", "å·¥ç¨‹ å¸«", "BY2", "å·¥ç¨‹å¸«"}
    stopwords = stopwords.union(custom_stopwords)

    # Clean lyrics
    def clean_and_tokenize_lyrics(track_record):
        for track in track_record:
            lyric = track["Lyrics"]
            tokens = jieba.lcut(lyric)
            clean_tokens = [word for word in tokens if word not in stopwords and word.strip()]
            track["Tokens"] = clean_tokens
        documents = [' '.join(track["Tokens"]) for track in track_record]
        return documents

    def clean_and_tokenize_input(text: str) -> str:
        tokens = jieba.lcut(text)
        clean_tokens = [word for word in tokens if word not in stopwords and word.strip()]
        return ' '.join(clean_tokens)

    def predict_cluster(text: str) -> str:
        cleaned = clean_and_tokenize_input(text)
        print(f"[DEBUG] æ–·è©çµæœ: {cleaned}")

        vec = vectorizer.transform([cleaned])
        print(f"[DEBUG] å‘é‡ç¶­åº¦: {vec.shape}")
        print(f"[DEBUG] éé›¶è©æ•¸ (nnz): {vec.nnz}")

        cluster_id = kmeans.predict(vec)[0]
        print(f"[DEBUG] é æ¸¬ç¾¤è™Ÿ: {cluster_id}")

        return f"é€™æ®µæ­Œè©å±¬æ–¼ç¬¬ {cluster_id} ç¾¤"


    from collections import Counter

    def generate_wordcloud_image(text: str = None, tokens: list[str] = None, title: str = None) -> Union[BytesIO, str]:
        """
        ç”¢ç”Ÿè©é›²åœ–åƒï¼Œå¯ä»¥å‚³å…¥åŸå§‹æ–‡å­—ï¼ˆtextï¼‰æˆ–å·²æ–·è©çš„ tokensã€‚
        è‹¥ç„¡æœ‰æ•ˆè©å½™ï¼Œå›å‚³è­¦å‘Šå­—ä¸²ï¼Œé¿å… ValueErrorã€‚
        """
        # è‹¥æœªæä¾› tokens å‰‡å¾ text è‡ªå‹•æ–·è©
        if tokens is None:
            if not text:
                return "âš ï¸ æ²’æœ‰è¼¸å…¥æ–‡å­—å…§å®¹ã€‚"
            tokens = jieba.lcut(text)

        # ç§»é™¤åœç”¨è©
        clean_tokens = [word for word in tokens if word not in stopwords and word.strip()]
        
        if not clean_tokens:
            return "âš ï¸ æ²’æœ‰æœ‰æ•ˆçš„è©å½™å¯ç”¢ç”Ÿè©é›²ï¼Œè«‹ç¢ºèªè¼¸å…¥æˆ–ç¾¤è™Ÿæ˜¯å¦æ­£ç¢ºã€‚"

        freq_dict = dict(Counter(clean_tokens))

        wc = WordCloud(
            width=1500,
            height=1500,
            background_color='white',
            max_words=200,
            font_path="TaipeiSansTCBeta-Regular.ttf",
            random_state=50,
            contour_width=1,
            contour_color='black',
            colormap='copper',
            prefer_horizontal=0.9
        )

        wc.generate_from_frequencies(freq_dict)

        fig, ax = plt.subplots(figsize=(6, 6), dpi=150)
        ax.imshow(wc, interpolation="bilinear")
        ax.axis("off")

        if title:
            ax.set_title(title, fontsize=24, pad=10)

        buf = BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight')
        plt.close(fig)
        buf.seek(0)

        return buf


    def show_cluster_wordcloud(cluster_id: int):
        # åˆ¤æ–·ç›®å‰ç¾¤çš„ç¯„åœ
        cluster_ids = set(track.get("Cluster") for track in full_record if "Cluster" in track)
        max_cluster_id = max(cluster_ids)

        if cluster_id > max_cluster_id or cluster_id < 0:
            return f"âš ï¸ ç›®å‰å…±æœ‰ {max_cluster_id + 1} ç¾¤ï¼Œæ‰¾ä¸åˆ°ç¬¬ {cluster_id} ç¾¤ï¼Œè«‹ç¢ºèªç¾¤è™Ÿæ˜¯å¦æ­£ç¢ºã€‚"

        # æ”¶é›†è©²ç¾¤çš„ tokens
        cluster_tokens = []
        for track in full_record:
            if track.get("Cluster") == cluster_id and "Tokens" in track:
                cluster_tokens.extend(track["Tokens"])

        if not cluster_tokens:
            return f"âš ï¸ ç¬¬ {cluster_id} ç¾¤ä¸­æ²’æœ‰æœ‰æ•ˆçš„è©å½™å¯ç”¢ç”Ÿè©é›²ã€‚"

        # å‚³å…¥ tokens è€Œä¸æ˜¯ textï¼Œé¿å…é‡æ–°æ–·è©éŒ¯èª¤
        return generate_wordcloud_image(tokens=cluster_tokens, title=f"ç¬¬ {cluster_id} ç¾¤ è©é›²")

    def summarize_clusters() -> str:
        summary = []
        cluster_counts = {}
        token_counts = {}

        for track in full_record:
            cid = track.get("Cluster")
            if cid is None:
                continue
            cluster_counts[cid] = cluster_counts.get(cid, 0) + 1
            token_counts[cid] = token_counts.get(cid, 0) + len(track.get("Tokens", []))

        for cid in sorted(cluster_counts):
            summary.append(f"ğŸ”¹ ç¬¬ {cid} ç¾¤ï¼š{cluster_counts[cid]} é¦–æ­Œï¼Œ{token_counts[cid]} å€‹è©")

        if not summary:
            return "âš ï¸ å°šæœªå®Œæˆåˆ†ç¾¤ï¼Œè«‹ç¢ºèªæ¨¡å‹å·²è¨“ç·´ã€‚"

        return "ğŸ“Š åˆ†ç¾¤æ‘˜è¦ï¼š\n" + "\n".join(summary)


    def generate_response(prompt):
        prompt = prompt.strip()
        print(f"[DEBUG] ä½¿ç”¨è€…è¼¸å…¥ï¼š{prompt}")

        # ç¾¤è©é›²é‚è¼¯æ”¾æœ€å‰ï¼Œé¿å…è¢« "ç•«...è©é›²" æ””æˆª
        if re.search(r"ç¬¬\s*\d+\s*ç¾¤.*è©é›²", prompt):
            print("[DEBUG] é€²å…¥ç¾¤è©é›²é¡¯ç¤ºé‚è¼¯")
            match = re.search(r"ç¬¬\s*(\d+)\s*ç¾¤", prompt)
            if match:
                cluster_id = int(match.group(1))
                return show_cluster_wordcloud(cluster_id)
            else:
                return "è«‹æä¾›æœ‰æ•ˆçš„ç¾¤è™Ÿï¼ˆä¾‹å¦‚ï¼šç¬¬ 2 ç¾¤ï¼‰ã€‚"

        elif re.search(r"(ç•«|ç”Ÿæˆ|çµ¦æˆ‘).*è©é›²", prompt):
            print("[DEBUG] é€²å…¥è©é›²ç”Ÿæˆé‚è¼¯")
            text = prompt.split('è©é›²')[-1].strip(":ï¼š ")
            return generate_wordcloud_image(text)

        elif "åˆ†é¡" in prompt or "é€™æ®µæ­Œè©å±¬æ–¼å“ªä¸€ç¾¤" in prompt:
            print("[DEBUG] é€²å…¥åˆ†ç¾¤é æ¸¬é‚è¼¯")
            text = prompt.split("æ­Œè©")[-1].strip(":ï¼š ")
            return predict_cluster(text)

        elif "å¹¾ç¾¤" in prompt or "ç›®å‰æœ‰å¹¾ç¾¤" in prompt or "ç¾¤æ‘˜è¦" in prompt:
            print("[DEBUG] é€²å…¥åˆ†ç¾¤æ‘˜è¦é‚è¼¯")
            return summarize_clusters()

        else:
            print("[DEBUG] è½å…¥ fallback å›å‚³å€")
            return f"You say: {prompt}."


    def chat(prompt: str):
        st_c_chat.chat_message("user", avatar=user_image).write(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        response = generate_response(prompt)
        st.session_state.messages.append({"role": "assistant", "content": response})

        if isinstance(response, BytesIO):
            st_c_chat.chat_message("assistant").image(response)
        else:
            st_c_chat.chat_message("assistant").write_stream(stream_data(response))

    # Prepare model
    cleaned_docs = clean_and_tokenize_lyrics(full_record)
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(cleaned_docs)
    kmeans = KMeans(n_clusters=4, random_state=42)
    kmeans.fit(tfidf_matrix)

    labels = kmeans.labels_
    for i, track in enumerate(full_record):
        track["Cluster"] = int(labels[i])

    if prompt := st.chat_input(placeholder=placeholderstr, key="chat_bot"):
        chat(prompt)

if __name__ == "__main__":
    main()
